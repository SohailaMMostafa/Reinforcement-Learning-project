{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP10AP7MDFoLcOC87pHInN4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dg8v_O_UI9vi","executionInfo":{"status":"ok","timestamp":1715683156143,"user_tz":-180,"elapsed":15468,"user":{"displayName":"Sohaila Mostafa","userId":"00523773491357675957"}},"outputId":"7c882680-ad39-424d-efb6-00be5ffe2445"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gymnasium\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/953.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/953.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.0/953.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m952.3/953.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n","Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (2.5.2)\n"]}],"source":["!pip install gymnasium\n","!pip install pygame"]},{"cell_type":"code","source":["import numpy as np\n","import gym\n","import math\n","from IPython import display"],"metadata":{"id":"cRoRT37pcfV1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setup the environment\n","env = gym.make(\"CartPole-v1\")"],"metadata":{"id":"Rb7G7EAwcuol","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715683165977,"user_tz":-180,"elapsed":50,"user":{"displayName":"Sohaila Mostafa","userId":"00523773491357675957"}},"outputId":"3ce0c407-76e7-4cc7-a69a-828c122fd920"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}]},{"cell_type":"code","source":["alpha = 0.1\n","gamma = 0.9\n","num_episodes = 150000\n","\n","epsilon = 0.9\n","epsilon_decay_value = 0.99995\n","\n","total_reward = 0\n","prior_reward = 0"],"metadata":{"id":"5AdrGbODcy_9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Observation = [30, 30, 50, 50]\n","np_array_win_size = np.array([0.25, 0.25, 0.01, 0.1])"],"metadata":{"id":"hTy8338Fc8hb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["q_matrix = np.random.uniform(low=0, high=1, size=(Observation + [env.action_space.n]))"],"metadata":{"id":"9KPcS5kDdEoG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_discrete_state(state):\n","    discrete_state = state / np_array_win_size + np.array([15, 10, 1, 10])\n","    return tuple(discrete_state.astype(int))"],"metadata":{"id":"pQT_ZWWxdJzJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def epsilon_greedy_action_selection(epsilon, q_values):\n","    if np.random.random() < epsilon:\n","        return np.random.randint(0, len(q_values))\n","    else:\n","        return np.argmax(q_values)"],"metadata":{"id":"z2wJUyH1cYe7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["returns_sum = np.zeros(q_matrix.shape)\n","returns_count = np.zeros(q_matrix.shape)"],"metadata":{"id":"fujmk2qqcO0E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Monte Carlo implementation\n","\n","for episode in range(num_episodes +1):\n","    episode_transitions = []\n","    state = env.reset()\n","    done = False\n","    episode_reward = 0  # Store reward per episode\n","\n","    while not done:\n","        state_index = get_discrete_state(state)\n","        action = epsilon_greedy_action_selection(0.1, q_matrix[state_index])\n","        next_state, reward, done, _ = env.step(action)\n","        episode_transitions.append((state_index, action, reward))\n","        episode_reward += reward\n","        state = next_state\n","\n","    # Accumulate rewards and update average\n","    total_reward += episode_reward\n","\n","    # Monte Carlo update at the end of the episode\n","    G = 0\n","    for transition in reversed(episode_transitions):\n","        state, action, reward = transition\n","        G = reward + gamma * G\n","        returns_sum[state + (action,)] += G\n","        returns_count[state + (action,)] += 1\n","        q_matrix[state + (action,)] = returns_sum[state + (action,)] / returns_count[state + (action,)]\n","\n","    # Calculate and print average reward every 'print_every' episodes\n","    if episode % 100 == 0:\n","        mean_reward = total_reward / 100\n","        print(f\"Episode: {episode}, Mean Reward: {mean_reward}, epsilon: {epsilon}\")\n","        total_reward = 0\n","        if(mean_reward > 195.0):\n","          print(\"Problem Solved!\")\n","          break\n","        total_reward = 0  # Reset total rewards after printing"],"metadata":{"id":"W4OABjP9U5p8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715612715428,"user_tz":-180,"elapsed":31112,"user":{"displayName":"Sohaila Mostafa","userId":"00523773491357675957"}},"outputId":"352c76d1-8c62-4fb6-98a9-1ad75bfcaed7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode: 0, Mean Reward: 0.22, epsilon: 0.9\n","Episode: 100, Mean Reward: 22.67, epsilon: 0.9\n","Episode: 200, Mean Reward: 31.21, epsilon: 0.9\n","Episode: 300, Mean Reward: 31.56, epsilon: 0.9\n","Episode: 400, Mean Reward: 35.42, epsilon: 0.9\n","Episode: 500, Mean Reward: 37.19, epsilon: 0.9\n","Episode: 600, Mean Reward: 41.44, epsilon: 0.9\n","Episode: 700, Mean Reward: 51.26, epsilon: 0.9\n","Episode: 800, Mean Reward: 55.1, epsilon: 0.9\n","Episode: 900, Mean Reward: 72.72, epsilon: 0.9\n","Episode: 1000, Mean Reward: 71.82, epsilon: 0.9\n","Episode: 1100, Mean Reward: 76.94, epsilon: 0.9\n","Episode: 1200, Mean Reward: 91.77, epsilon: 0.9\n","Episode: 1300, Mean Reward: 86.43, epsilon: 0.9\n","Episode: 1400, Mean Reward: 99.54, epsilon: 0.9\n","Episode: 1500, Mean Reward: 95.92, epsilon: 0.9\n","Episode: 1600, Mean Reward: 99.69, epsilon: 0.9\n","Episode: 1700, Mean Reward: 99.25, epsilon: 0.9\n","Episode: 1800, Mean Reward: 107.36, epsilon: 0.9\n","Episode: 1900, Mean Reward: 105.81, epsilon: 0.9\n","Episode: 2000, Mean Reward: 110.74, epsilon: 0.9\n","Episode: 2100, Mean Reward: 118.24, epsilon: 0.9\n","Episode: 2200, Mean Reward: 123.57, epsilon: 0.9\n","Episode: 2300, Mean Reward: 119.69, epsilon: 0.9\n","Episode: 2400, Mean Reward: 121.9, epsilon: 0.9\n","Episode: 2500, Mean Reward: 126.47, epsilon: 0.9\n","Episode: 2600, Mean Reward: 127.72, epsilon: 0.9\n","Episode: 2700, Mean Reward: 125.51, epsilon: 0.9\n","Episode: 2800, Mean Reward: 115.56, epsilon: 0.9\n","Episode: 2900, Mean Reward: 129.52, epsilon: 0.9\n","Episode: 3000, Mean Reward: 138.12, epsilon: 0.9\n","Episode: 3100, Mean Reward: 143.02, epsilon: 0.9\n","Episode: 3200, Mean Reward: 140.54, epsilon: 0.9\n","Episode: 3300, Mean Reward: 147.43, epsilon: 0.9\n","Episode: 3400, Mean Reward: 142.61, epsilon: 0.9\n","Episode: 3500, Mean Reward: 152.0, epsilon: 0.9\n","Episode: 3600, Mean Reward: 149.2, epsilon: 0.9\n","Episode: 3700, Mean Reward: 162.4, epsilon: 0.9\n","Episode: 3800, Mean Reward: 155.92, epsilon: 0.9\n","Episode: 3900, Mean Reward: 158.17, epsilon: 0.9\n","Episode: 4000, Mean Reward: 156.54, epsilon: 0.9\n","Episode: 4100, Mean Reward: 160.08, epsilon: 0.9\n","Episode: 4200, Mean Reward: 157.7, epsilon: 0.9\n","Episode: 4300, Mean Reward: 157.97, epsilon: 0.9\n","Episode: 4400, Mean Reward: 161.06, epsilon: 0.9\n","Episode: 4500, Mean Reward: 153.53, epsilon: 0.9\n","Episode: 4600, Mean Reward: 163.94, epsilon: 0.9\n","Episode: 4700, Mean Reward: 162.45, epsilon: 0.9\n","Episode: 4800, Mean Reward: 156.36, epsilon: 0.9\n","Episode: 4900, Mean Reward: 159.82, epsilon: 0.9\n","Episode: 5000, Mean Reward: 163.16, epsilon: 0.9\n","Episode: 5100, Mean Reward: 170.59, epsilon: 0.9\n","Episode: 5200, Mean Reward: 165.0, epsilon: 0.9\n","Episode: 5300, Mean Reward: 174.42, epsilon: 0.9\n","Episode: 5400, Mean Reward: 180.02, epsilon: 0.9\n","Episode: 5500, Mean Reward: 182.25, epsilon: 0.9\n","Episode: 5600, Mean Reward: 178.39, epsilon: 0.9\n","Episode: 5700, Mean Reward: 167.39, epsilon: 0.9\n","Episode: 5800, Mean Reward: 175.47, epsilon: 0.9\n","Episode: 5900, Mean Reward: 185.07, epsilon: 0.9\n","Episode: 6000, Mean Reward: 183.56, epsilon: 0.9\n","Episode: 6100, Mean Reward: 194.43, epsilon: 0.9\n","Episode: 6200, Mean Reward: 178.87, epsilon: 0.9\n","Episode: 6300, Mean Reward: 186.67, epsilon: 0.9\n","Episode: 6400, Mean Reward: 193.96, epsilon: 0.9\n","Episode: 6500, Mean Reward: 196.33, epsilon: 0.9\n","Problem Solved!\n"]}]},{"cell_type":"code","source":["# Q_Learning implementation\n","\n","for episode in range(num_episodes + 1):\n","    state_index = get_discrete_state(env.reset())\n","    done = False\n","    episode_reward = 0\n","\n","    while not done:\n","        if np.random.random() > epsilon:\n","            action = np.argmax(q_matrix[state_index])\n","        else:\n","            action = np.random.randint(0, env.action_space.n)\n","\n","        new_state, reward, done, _ = env.step(action)\n","        episode_reward += reward\n","        new_state_index = get_discrete_state(new_state)\n","\n","        if not done:\n","            max_next_q = np.max(q_matrix[new_state_index])\n","            current_q = q_matrix[state_index + (action,)]\n","            error = reward + gamma * (max_next_q - current_q)\n","            new_q = q_matrix[state_index + (action,)] + alpha * error\n","            q_matrix[state_index + (action,)] = new_q\n","\n","        state_index = new_state_index\n","\n","    if epsilon > 0.05:\n","        if episode_reward > prior_reward and episode > 100:\n","            epsilon = math.pow(epsilon_decay_value, episode - 100)\n","\n","    total_reward += episode_reward\n","    prior_reward = episode_reward\n","\n","    if episode % 100 == 0:\n","        mean_reward = total_reward / 100\n","        print(f\"Episode: {episode}, Mean Reward: {mean_reward}, epsilon: {epsilon}\")\n","        total_reward = 0\n","        if(mean_reward > 195.0):\n","          print(\"Problem Solved!\")\n","          break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s7WGienwU52L","executionInfo":{"status":"ok","timestamp":1715611824140,"user_tz":-180,"elapsed":88039,"user":{"displayName":"Sohaila Mostafa","userId":"00523773491357675957"}},"outputId":"ea6f218a-4720-4019-aeda-6a69d4563087"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode: 0, Mean Reward: 1.66, epsilon: 0.2850669038119228\n","Episode: 100, Mean Reward: 189.15, epsilon: 0.2850669038119228\n","Episode: 200, Mean Reward: 29.88, epsilon: 0.9950123548119847\n","Episode: 300, Mean Reward: 24.15, epsilon: 0.9900495862284909\n","Episode: 400, Mean Reward: 23.58, epsilon: 0.9851115701738419\n","Episode: 500, Mean Reward: 21.94, epsilon: 0.9802471955509836\n","Episode: 600, Mean Reward: 24.76, epsilon: 0.9753093024395111\n","Episode: 700, Mean Reward: 22.29, epsilon: 0.9705418574497624\n","Episode: 800, Mean Reward: 22.84, epsilon: 0.9656045713250361\n","Episode: 900, Mean Reward: 24.22, epsilon: 0.9607884783313412\n","Episode: 1000, Mean Reward: 24.91, epsilon: 0.9560920131117725\n","Episode: 1100, Mean Reward: 24.54, epsilon: 0.9512282354250458\n","Episode: 1200, Mean Reward: 26.78, epsilon: 0.9464838464939238\n","Episode: 1300, Mean Reward: 23.19, epsilon: 0.9418573042672076\n","Episode: 1400, Mean Reward: 24.18, epsilon: 0.9370659405932598\n","Episode: 1500, Mean Reward: 26.11, epsilon: 0.9323921881638068\n","Episode: 1600, Mean Reward: 24.35, epsilon: 0.9277417467531685\n","Episode: 1700, Mean Reward: 27.35, epsilon: 0.9231606581271604\n","Episode: 1800, Mean Reward: 27.21, epsilon: 0.9185562603128875\n","Episode: 1900, Mean Reward: 24.0, epsilon: 0.9140205286276479\n","Episode: 2000, Mean Reward: 28.13, epsilon: 0.9093707746380917\n","Episode: 2100, Mean Reward: 26.46, epsilon: 0.9049708947168852\n","Episode: 2200, Mean Reward: 26.3, epsilon: 0.900322159158725\n","Episode: 2300, Mean Reward: 28.5, epsilon: 0.8958316716739334\n","Episode: 2400, Mean Reward: 28.94, epsilon: 0.8913635811474372\n","Episode: 2500, Mean Reward: 29.09, epsilon: 0.887006474301069\n","Episode: 2600, Mean Reward: 26.94, epsilon: 0.8824941446941661\n","Episode: 2700, Mean Reward: 28.23, epsilon: 0.8781803928637659\n","Episode: 2800, Mean Reward: 26.7, epsilon: 0.8737129628035252\n","Episode: 2900, Mean Reward: 30.12, epsilon: 0.8693986624820157\n","Episode: 3000, Mean Reward: 27.36, epsilon: 0.865019157306099\n","Episode: 3100, Mean Reward: 27.04, epsilon: 0.8607047486686201\n","Episode: 3200, Mean Reward: 31.33, epsilon: 0.8564546815046964\n","Episode: 3300, Mean Reward: 28.92, epsilon: 0.8521829894337364\n","Episode: 3400, Mean Reward: 28.75, epsilon: 0.8479326030471787\n","Episode: 3500, Mean Reward: 33.26, epsilon: 0.8436612309090251\n","Episode: 3600, Mean Reward: 30.64, epsilon: 0.8394953227965065\n","Episode: 3700, Mean Reward: 32.86, epsilon: 0.8352664525784996\n","Episode: 3800, Mean Reward: 30.03, epsilon: 0.8311004398755859\n","Episode: 3900, Mean Reward: 30.53, epsilon: 0.8269965555936627\n","Episode: 4000, Mean Reward: 32.92, epsilon: 0.8228717902026508\n","Episode: 4100, Mean Reward: 30.98, epsilon: 0.818726659298009\n","Episode: 4200, Mean Reward: 33.17, epsilon: 0.8146431412154616\n","Episode: 4300, Mean Reward: 33.8, epsilon: 0.8105799902722285\n","Episode: 4400, Mean Reward: 34.43, epsilon: 0.8065774337559336\n","Episode: 4500, Mean Reward: 34.62, epsilon: 0.802514383974114\n","Episode: 4600, Mean Reward: 32.99, epsilon: 0.7985516545513001\n","Episode: 4700, Mean Reward: 34.36, epsilon: 0.7945290337959839\n","Episode: 4800, Mean Reward: 32.45, epsilon: 0.7905662048838329\n","Episode: 4900, Mean Reward: 33.11, epsilon: 0.7866231411562365\n","Episode: 5000, Mean Reward: 37.45, epsilon: 0.7827388809755159\n","Episode: 5100, Mean Reward: 38.67, epsilon: 0.7787959154194878\n","Episode: 5200, Mean Reward: 35.42, epsilon: 0.7749115577194998\n","Episode: 5300, Mean Reward: 37.51, epsilon: 0.7711236842581193\n","Episode: 5400, Mean Reward: 39.0, epsilon: 0.7672008670838661\n","Episode: 5500, Mean Reward: 40.26, epsilon: 0.7633743413709142\n","Episode: 5600, Mean Reward: 41.7, epsilon: 0.7596048812545839\n","Episode: 5700, Mean Reward: 35.22, epsilon: 0.7557784508117203\n","Episode: 5800, Mean Reward: 39.38, epsilon: 0.7520088960583236\n","Episode: 5900, Mean Reward: 35.78, epsilon: 0.7483329739331145\n","Episode: 6000, Mean Reward: 42.97, epsilon: 0.7445260963826875\n","Episode: 6100, Mean Reward: 44.02, epsilon: 0.7408126643807126\n","Episode: 6200, Mean Reward: 41.77, epsilon: 0.7371546113905628\n","Episode: 6300, Mean Reward: 38.88, epsilon: 0.7334412718429504\n","Episode: 6400, Mean Reward: 39.14, epsilon: 0.7299291018845564\n","Episode: 6500, Mean Reward: 40.07, epsilon: 0.7262884745119496\n","Episode: 6600, Mean Reward: 39.71, epsilon: 0.7225214829355084\n","Episode: 6700, Mean Reward: 42.07, epsilon: 0.7189178021379075\n","Episode: 6800, Mean Reward: 37.88, epsilon: 0.7153678636146765\n","Episode: 6900, Mean Reward: 43.63, epsilon: 0.711799862532058\n","Episode: 7000, Mean Reward: 38.12, epsilon: 0.7082142448900014\n","Episode: 7100, Mean Reward: 45.03, epsilon: 0.7047523969972106\n","Episode: 7200, Mean Reward: 44.29, epsilon: 0.701167220114469\n","Episode: 7300, Mean Reward: 45.92, epsilon: 0.697670046803071\n","Episode: 7400, Mean Reward: 48.69, epsilon: 0.6942250274026814\n","Episode: 7500, Mean Reward: 45.94, epsilon: 0.6907624792853566\n","Episode: 7600, Mean Reward: 49.04, epsilon: 0.687282835269431\n","Episode: 7700, Mean Reward: 45.43, epsilon: 0.683854912343294\n","Episode: 7800, Mean Reward: 43.57, epsilon: 0.6804781105859735\n","Episode: 7900, Mean Reward: 49.72, epsilon: 0.6770502730057991\n","Episode: 8000, Mean Reward: 48.3, epsilon: 0.6736733864695973\n","Episode: 8100, Mean Reward: 44.53, epsilon: 0.6703133426452782\n","Episode: 8200, Mean Reward: 49.48, epsilon: 0.666970057527371\n","Episode: 8300, Mean Reward: 43.79, epsilon: 0.6636434475293943\n","Episode: 8400, Mean Reward: 59.3, epsilon: 0.6603334294817664\n","Episode: 8500, Mean Reward: 48.77, epsilon: 0.6570727742684396\n","Episode: 8600, Mean Reward: 47.5, epsilon: 0.6537955284076836\n","Episode: 8700, Mean Reward: 49.16, epsilon: 0.6505671566443074\n","Episode: 8800, Mean Reward: 48.74, epsilon: 0.6472576278784459\n","Episode: 8900, Mean Reward: 53.67, epsilon: 0.6440293364853518\n","Episode: 9000, Mean Reward: 58.52, epsilon: 0.6408171466642899\n","Episode: 9100, Mean Reward: 62.96, epsilon: 0.6376528607493696\n","Episode: 9200, Mean Reward: 59.08, epsilon: 0.6344724745268288\n","Episode: 9300, Mean Reward: 52.73, epsilon: 0.6313079509423268\n","Episode: 9400, Mean Reward: 59.06, epsilon: 0.6281278029181097\n","Episode: 9500, Mean Reward: 52.55, epsilon: 0.6249949243044265\n","Episode: 9600, Mean Reward: 58.73, epsilon: 0.6219087668160264\n","Episode: 9700, Mean Reward: 57.47, epsilon: 0.6188378484402538\n","Episode: 9800, Mean Reward: 57.09, epsilon: 0.615689731232215\n","Episode: 9900, Mean Reward: 67.14, epsilon: 0.6126188893069242\n","Episode: 10000, Mean Reward: 56.33, epsilon: 0.6095938433437524\n","Episode: 10100, Mean Reward: 62.26, epsilon: 0.6065837347310917\n","Episode: 10200, Mean Reward: 55.53, epsilon: 0.6035281323699176\n","Episode: 10300, Mean Reward: 59.42, epsilon: 0.600517948184671\n","Episode: 10400, Mean Reward: 58.38, epsilon: 0.597552655362859\n","Episode: 10500, Mean Reward: 64.06, epsilon: 0.5945425461230158\n","Episode: 10600, Mean Reward: 64.93, epsilon: 0.591577178853775\n","Episode: 10700, Mean Reward: 60.33, epsilon: 0.5887149046047278\n","Episode: 10800, Mean Reward: 60.18, epsilon: 0.5857200271477553\n","Episode: 10900, Mean Reward: 67.05, epsilon: 0.5827403850634769\n","Episode: 11000, Mean Reward: 71.72, epsilon: 0.5798628759298494\n","Episode: 11100, Mean Reward: 62.08, epsilon: 0.5769707256470092\n","Episode: 11200, Mean Reward: 62.21, epsilon: 0.5740930003836102\n","Episode: 11300, Mean Reward: 65.01, epsilon: 0.5712296281927736\n","Episode: 11400, Mean Reward: 71.01, epsilon: 0.5683805374864661\n","Episode: 11500, Mean Reward: 63.5, epsilon: 0.5655456570337102\n","Episode: 11600, Mean Reward: 65.31, epsilon: 0.5627530536114836\n","Episode: 11700, Mean Reward: 69.91, epsilon: 0.5598902478273579\n","Episode: 11800, Mean Reward: 71.33, epsilon: 0.5571255702054753\n","Episode: 11900, Mean Reward: 71.61, epsilon: 0.5543191081948429\n","Episode: 12000, Mean Reward: 67.06, epsilon: 0.55155436116223\n","Episode: 12100, Mean Reward: 68.17, epsilon: 0.5488034037068503\n","Episode: 12200, Mean Reward: 68.99, epsilon: 0.5460934717247716\n","Episode: 12300, Mean Reward: 72.05, epsilon: 0.543369751248317\n","Episode: 12400, Mean Reward: 76.47, epsilon: 0.5406325827424041\n","Episode: 12500, Mean Reward: 74.77, epsilon: 0.5379629973924743\n","Episode: 12600, Mean Reward: 74.53, epsilon: 0.5352530648457575\n","Episode: 12700, Mean Reward: 75.36, epsilon: 0.532610042974658\n","Episode: 12800, Mean Reward: 74.98, epsilon: 0.5299270753780739\n","Episode: 12900, Mean Reward: 77.6, epsilon: 0.5273103526681988\n","Episode: 13000, Mean Reward: 80.01, epsilon: 0.5246540817093364\n","Episode: 13100, Mean Reward: 66.88, epsilon: 0.5220372933033263\n","Episode: 13200, Mean Reward: 77.7, epsilon: 0.5194335565094174\n","Episode: 13300, Mean Reward: 78.21, epsilon: 0.5168686496632826\n","Episode: 13400, Mean Reward: 74.3, epsilon: 0.5142649776953421\n","Episode: 13500, Mean Reward: 83.03, epsilon: 0.5117000064539752\n","Episode: 13600, Mean Reward: 80.82, epsilon: 0.5091478283790776\n","Episode: 13700, Mean Reward: 88.82, epsilon: 0.5066337113484416\n","Episode: 13800, Mean Reward: 89.37, epsilon: 0.5041068021559483\n","Episode: 13900, Mean Reward: 81.38, epsilon: 0.5015674166651148\n","Episode: 14000, Mean Reward: 73.55, epsilon: 0.4990657763529198\n","Episode: 14100, Mean Reward: 84.64, epsilon: 0.4966511072762603\n","Episode: 14200, Mean Reward: 89.42, epsilon: 0.49412457160758927\n","Episode: 14300, Mean Reward: 80.89, epsilon: 0.4916846377976204\n","Episode: 14400, Mean Reward: 93.58, epsilon: 0.4892567521174939\n","Episode: 14500, Mean Reward: 94.31, epsilon: 0.4867678325978292\n","Episode: 14600, Mean Reward: 90.89, epsilon: 0.484340007359892\n","Episode: 14700, Mean Reward: 88.27, epsilon: 0.48192429125282016\n","Episode: 14800, Mean Reward: 100.3, epsilon: 0.47954460111062086\n","Episode: 14900, Mean Reward: 88.17, epsilon: 0.4771050887010559\n","Episode: 15000, Mean Reward: 90.74, epsilon: 0.4747491952609815\n","Episode: 15100, Mean Reward: 89.76, epsilon: 0.47235769565598784\n","Episode: 15200, Mean Reward: 86.58, epsilon: 0.4700017430682273\n","Episode: 15300, Mean Reward: 96.45, epsilon: 0.46768092518231336\n","Episode: 15400, Mean Reward: 99.16, epsilon: 0.4653250312513679\n","Episode: 15500, Mean Reward: 95.81, epsilon: 0.46300415509838394\n","Episode: 15600, Mean Reward: 98.59, epsilon: 0.4606948546521764\n","Episode: 15700, Mean Reward: 100.25, epsilon: 0.4583970721772271\n","Episode: 15800, Mean Reward: 107.78, epsilon: 0.456110750225982\n","Episode: 15900, Mean Reward: 92.71, epsilon: 0.45385852456364356\n","Episode: 16000, Mean Reward: 102.13, epsilon: 0.4515722595356001\n","Episode: 16100, Mean Reward: 96.91, epsilon: 0.44931997732828616\n","Episode: 16200, Mean Reward: 104.44, epsilon: 0.4470789287054856\n","Episode: 16300, Mean Reward: 113.41, epsilon: 0.44484905763806465\n","Episode: 16400, Mean Reward: 103.98, epsilon: 0.442630308376343\n","Episode: 16500, Mean Reward: 104.93, epsilon: 0.44044464768108404\n","Episode: 16600, Mean Reward: 95.14, epsilon: 0.43822595366018774\n","Episode: 16700, Mean Reward: 111.54, epsilon: 0.43604023809115106\n","Episode: 16800, Mean Reward: 114.58, epsilon: 0.4339088138924719\n","Episode: 16900, Mean Reward: 119.22, epsilon: 0.43176621899577344\n","Episode: 17000, Mean Reward: 98.4, epsilon: 0.4295697620930543\n","Episode: 17100, Mean Reward: 114.49, epsilon: 0.427427220536234\n","Episode: 17200, Mean Reward: 109.23, epsilon: 0.4252741004482389\n","Episode: 17300, Mean Reward: 104.34, epsilon: 0.4231529841275507\n","Episode: 17400, Mean Reward: 112.43, epsilon: 0.42106350035749046\n","Episode: 17500, Mean Reward: 102.93, epsilon: 0.4189843342327952\n","Episode: 17600, Mean Reward: 111.53, epsilon: 0.4169154348060455\n","Episode: 17700, Mean Reward: 117.57, epsilon: 0.41477378625376243\n","Episode: 17800, Mean Reward: 117.84, epsilon: 0.41270504177463896\n","Episode: 17900, Mean Reward: 120.92, epsilon: 0.410646615458962\n","Episode: 18000, Mean Reward: 123.11, epsilon: 0.4086188867877328\n","Episode: 18100, Mean Reward: 115.6, epsilon: 0.4065605117212756\n","Episode: 18200, Mean Reward: 117.21, epsilon: 0.40457318844876383\n","Episode: 18300, Mean Reward: 125.8, epsilon: 0.4025150664064924\n","Episode: 18400, Mean Reward: 142.88, epsilon: 0.40050746407242643\n","Episode: 18500, Mean Reward: 112.94, epsilon: 0.3985098749464814\n","Episode: 18600, Mean Reward: 126.07, epsilon: 0.39658173337202035\n","Episode: 18700, Mean Reward: 134.89, epsilon: 0.39458399421169277\n","Episode: 18800, Mean Reward: 108.17, epsilon: 0.39257668863830963\n","Episode: 18900, Mean Reward: 127.54, epsilon: 0.3906186554062958\n","Episode: 19000, Mean Reward: 128.68, epsilon: 0.3887092581033468\n","Episode: 19100, Mean Reward: 140.29, epsilon: 0.38675117571691847\n","Episode: 19200, Mean Reward: 128.18, epsilon: 0.38482219807639473\n","Episode: 19300, Mean Reward: 146.05, epsilon: 0.3829219875912971\n","Episode: 19400, Mean Reward: 136.65, epsilon: 0.3809740083241741\n","Episode: 19500, Mean Reward: 132.49, epsilon: 0.37907384514479714\n","Episode: 19600, Mean Reward: 144.83, epsilon: 0.3771831593051582\n","Episode: 19700, Mean Reward: 151.74, epsilon: 0.37530190353564946\n","Episode: 19800, Mean Reward: 137.42, epsilon: 0.37343003080242687\n","Episode: 19900, Mean Reward: 135.38, epsilon: 0.37156749430623476\n","Episode: 20000, Mean Reward: 152.44, epsilon: 0.3697327341179413\n","Episode: 20100, Mean Reward: 135.14, epsilon: 0.3678702439938449\n","Episode: 20200, Mean Reward: 144.3, epsilon: 0.3660537404285964\n","Episode: 20300, Mean Reward: 139.89, epsilon: 0.3642097828518801\n","Episode: 20400, Mean Reward: 151.26, epsilon: 0.3624113542487233\n","Episode: 20500, Mean Reward: 143.93, epsilon: 0.3606218060919271\n","Episode: 20600, Mean Reward: 150.28, epsilon: 0.3587872710578896\n","Episode: 20700, Mean Reward: 137.2, epsilon: 0.3569977674518766\n","Episode: 20800, Mean Reward: 136.76, epsilon: 0.35523495100246316\n","Episode: 20900, Mean Reward: 155.53, epsilon: 0.35344549195022545\n","Episode: 21000, Mean Reward: 143.35, epsilon: 0.3516826312430742\n","Episode: 21100, Mean Reward: 159.92, epsilon: 0.3499285630596461\n","Episode: 21200, Mean Reward: 150.13, epsilon: 0.34818324354595254\n","Episode: 21300, Mean Reward: 138.83, epsilon: 0.346446629066733\n","Episode: 21400, Mean Reward: 151.93, epsilon: 0.3447359130000142\n","Episode: 21500, Mean Reward: 137.48, epsilon: 0.34303364426461685\n","Episode: 21600, Mean Reward: 162.11, epsilon: 0.3412885827413639\n","Episode: 21700, Mean Reward: 155.72, epsilon: 0.3395863563839294\n","Episode: 21800, Mean Reward: 164.8, epsilon: 0.33789262012759547\n","Episode: 21900, Mean Reward: 171.83, epsilon: 0.33620733162675015\n","Episode: 22000, Mean Reward: 174.81, epsilon: 0.33453044874698656\n","Episode: 22100, Mean Reward: 180.94, epsilon: 0.33287857349272365\n","Episode: 22200, Mean Reward: 179.64, epsilon: 0.3312182932774493\n","Episode: 22300, Mean Reward: 166.62, epsilon: 0.3295662939508014\n","Episode: 22400, Mean Reward: 165.08, epsilon: 0.32793893115720346\n","Episode: 22500, Mean Reward: 173.29, epsilon: 0.32627065861220006\n","Episode: 22600, Mean Reward: 157.8, epsilon: 0.32464333633178233\n","Episode: 22700, Mean Reward: 178.94, epsilon: 0.3230402825716345\n","Episode: 22800, Mean Reward: 158.66, epsilon: 0.321429072260731\n","Episode: 22900, Mean Reward: 164.48, epsilon: 0.319841890189691\n","Episode: 23000, Mean Reward: 168.55, epsilon: 0.3182307199935444\n","Episode: 23100, Mean Reward: 188.63, epsilon: 0.3166909993490494\n","Episode: 23200, Mean Reward: 169.61, epsilon: 0.31504843944517064\n","Episode: 23300, Mean Reward: 161.62, epsilon: 0.31349276425039274\n","Episode: 23400, Mean Reward: 187.81, epsilon: 0.31192917357330163\n","Episode: 23500, Mean Reward: 186.73, epsilon: 0.31037338153172717\n","Episode: 23600, Mean Reward: 188.39, epsilon: 0.30880990796138097\n","Episode: 23700, Mean Reward: 193.42, epsilon: 0.3072696737099259\n","Episode: 23800, Mean Reward: 186.99, epsilon: 0.30576769760576494\n","Episode: 23900, Mean Reward: 198.81, epsilon: 0.304242636820151\n","Problem Solved!\n"]}]},{"cell_type":"code","source":["# SARSA Learning\n","\n","for episode in range(num_episodes + 1):\n","    state_index = get_discrete_state(env.reset())\n","    done = False\n","    episode_reward = 0\n","\n","    # Choose action using epsilon-greedy strategy\n","    if np.random.random() > epsilon:\n","        action = np.argmax(q_matrix[state_index])\n","    else:\n","        action = np.random.randint(0, env.action_space.n)\n","\n","    while not done:\n","        new_state, reward, done, _ = env.step(action)\n","        episode_reward += reward\n","\n","        new_state_index = get_discrete_state(new_state)\n","\n","        # Choose the next action (again using epsilon-greedy)\n","        if np.random.random() > epsilon:\n","            new_action = np.argmax(q_matrix[new_state_index])\n","        else:\n","            new_action = np.random.randint(0, env.action_space.n)\n","\n","        # SARSA update\n","        current_q = q_matrix[state_index + (action,)]\n","        next_q = q_matrix[new_state_index + (new_action,)]\n","        error = reward + gamma * (next_q - current_q)\n","        new_q = q_matrix[state_index + (action,)] + alpha * error\n","        q_matrix[state_index + (action,)] = new_q\n","\n","        state_index = new_state_index\n","        action = new_action\n","\n","    # Epsilon decay\n","    if epsilon > 0.05:\n","        if episode_reward > prior_reward and episode > 100:\n","            epsilon = math.pow(epsilon_decay_value, episode - 100)\n","\n","    total_reward += episode_reward\n","    prior_reward = episode_reward\n","\n","\n","    if episode % 100 == 0:\n","        mean_reward = total_reward / 100\n","        print(f\"Episode: {episode}, Mean Reward: {mean_reward}, epsilon: {epsilon}\")\n","        total_reward = 0\n","        if(mean_reward > 195.0):\n","          print(\"Problem Solved!\")\n","          break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0qjONXRAdNCU","executionInfo":{"status":"ok","timestamp":1715611121603,"user_tz":-180,"elapsed":75147,"user":{"displayName":"Sohaila Mostafa","userId":"00523773491357675957"}},"outputId":"867f7e42-61d4-415f-8b88-d26dde811603"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode: 0, Mean Reward: 2.22, epsilon: 0.22425143588801186\n","Episode: 100, Mean Reward: 190.64, epsilon: 0.22425143588801186\n","Episode: 200, Mean Reward: 25.33, epsilon: 0.9950621079173806\n","Episode: 300, Mean Reward: 23.42, epsilon: 0.9901981085184066\n","Episode: 400, Mean Reward: 24.37, epsilon: 0.9851115701738419\n","Episode: 500, Mean Reward: 24.81, epsilon: 0.9801981831912061\n","Episode: 600, Mean Reward: 22.86, epsilon: 0.9753093024395111\n","Episode: 700, Mean Reward: 24.85, epsilon: 0.9704448056903722\n","Episode: 800, Mean Reward: 23.0, epsilon: 0.9656528539677345\n","Episode: 900, Mean Reward: 22.27, epsilon: 0.9608365201573491\n","Episode: 1000, Mean Reward: 24.82, epsilon: 0.9559964063006914\n","Episode: 1100, Mean Reward: 23.87, epsilon: 0.9512757992150065\n","Episode: 1200, Mean Reward: 25.22, epsilon: 0.9464838464939238\n","Episode: 1300, Mean Reward: 24.8, epsilon: 0.9417631208914241\n","Episode: 1400, Mean Reward: 23.96, epsilon: 0.9371127962330714\n","Episode: 1500, Mean Reward: 23.7, epsilon: 0.9323921881638068\n","Episode: 1600, Mean Reward: 26.32, epsilon: 0.9277417467531685\n","Episode: 1700, Mean Reward: 24.73, epsilon: 0.9231606581271604\n","Episode: 1800, Mean Reward: 28.32, epsilon: 0.9185103324998718\n","Episode: 1900, Mean Reward: 26.25, epsilon: 0.9139748276012166\n","Episode: 2000, Mean Reward: 24.87, epsilon: 0.9093707746380917\n","Episode: 2100, Mean Reward: 28.78, epsilon: 0.9048351558698463\n","Episode: 2200, Mean Reward: 28.74, epsilon: 0.900322159158725\n","Episode: 2300, Mean Reward: 26.87, epsilon: 0.8959212615602863\n","Episode: 2400, Mean Reward: 27.15, epsilon: 0.891408151555015\n","Episode: 2500, Mean Reward: 27.06, epsilon: 0.886962123977354\n","Episode: 2600, Mean Reward: 26.96, epsilon: 0.8825382716077464\n","Episode: 2700, Mean Reward: 29.74, epsilon: 0.8780925770199306\n","Episode: 2800, Mean Reward: 28.54, epsilon: 0.873756650636057\n","Episode: 2900, Mean Reward: 29.2, epsilon: 0.8693551925488916\n","Episode: 3000, Mean Reward: 28.04, epsilon: 0.8651056657099058\n","Episode: 3100, Mean Reward: 26.7, epsilon: 0.8607047486686201\n","Episode: 3200, Mean Reward: 28.22, epsilon: 0.8564975063800154\n","Episode: 3300, Mean Reward: 28.61, epsilon: 0.8521403802842646\n","Episode: 3400, Mean Reward: 31.0, epsilon: 0.8479326030471787\n","Episode: 3500, Mean Reward: 30.73, epsilon: 0.8437034160798291\n","Episode: 3600, Mean Reward: 33.26, epsilon: 0.8394953227965065\n","Episode: 3700, Mean Reward: 30.01, epsilon: 0.8353082179893991\n","Episode: 3800, Mean Reward: 31.43, epsilon: 0.8311835561532424\n","Episode: 3900, Mean Reward: 32.45, epsilon: 0.8269965555936627\n","Episode: 4000, Mean Reward: 31.22, epsilon: 0.8228306466131406\n","Episode: 4100, Mean Reward: 33.06, epsilon: 0.818767597677893\n","Episode: 4200, Mean Reward: 36.27, epsilon: 0.8146431412154616\n","Episode: 4300, Mean Reward: 32.77, epsilon: 0.8105799902722285\n","Episode: 4400, Mean Reward: 36.96, epsilon: 0.8065774337559336\n","Episode: 4500, Mean Reward: 31.87, epsilon: 0.8025946414317706\n","Episode: 4600, Mean Reward: 32.17, epsilon: 0.7985117269685725\n","Episode: 4700, Mean Reward: 32.48, epsilon: 0.7946084926587286\n","Episode: 4800, Mean Reward: 31.2, epsilon: 0.7905662048838329\n","Episode: 4900, Mean Reward: 35.85, epsilon: 0.7866624742799505\n","Episode: 5000, Mean Reward: 32.16, epsilon: 0.7826997440314671\n","Episode: 5100, Mean Reward: 32.09, epsilon: 0.7787959154194878\n","Episode: 5200, Mean Reward: 35.89, epsilon: 0.7749503052347616\n","Episode: 5300, Mean Reward: 36.09, epsilon: 0.7710851280739064\n","Episode: 5400, Mean Reward: 35.44, epsilon: 0.7672008670838661\n","Episode: 5500, Mean Reward: 37.66, epsilon: 0.7634125119965139\n","Episode: 5600, Mean Reward: 34.57, epsilon: 0.7595669010105212\n","Episode: 5700, Mean Reward: 37.15, epsilon: 0.7558162416238016\n","Episode: 5800, Mean Reward: 38.21, epsilon: 0.7520088960583236\n","Episode: 5900, Mean Reward: 40.77, epsilon: 0.7482955572844178\n","Episode: 6000, Mean Reward: 37.76, epsilon: 0.7445633245489149\n","Episode: 6100, Mean Reward: 39.8, epsilon: 0.7408126643807126\n","Episode: 6200, Mean Reward: 34.75, epsilon: 0.7371546113905628\n","Episode: 6300, Mean Reward: 37.02, epsilon: 0.7334779457402374\n","Episode: 6400, Mean Reward: 40.14, epsilon: 0.7298561107991908\n","Episode: 6500, Mean Reward: 43.67, epsilon: 0.7261432277110111\n","Episode: 6600, Mean Reward: 38.85, epsilon: 0.7225214829355084\n","Episode: 6700, Mean Reward: 38.54, epsilon: 0.7189178021379075\n","Episode: 6800, Mean Reward: 40.46, epsilon: 0.7153320952214958\n","Episode: 6900, Mean Reward: 42.65, epsilon: 0.711799862532058\n","Episode: 7000, Mean Reward: 44.53, epsilon: 0.7083204876508339\n","Episode: 7100, Mean Reward: 41.55, epsilon: 0.7046819235193919\n","Episode: 7200, Mean Reward: 42.46, epsilon: 0.701167220114469\n","Episode: 7300, Mean Reward: 43.56, epsilon: 0.6977398190406255\n","Episode: 7400, Mean Reward: 40.94, epsilon: 0.6942250274026814\n","Episode: 7500, Mean Reward: 42.15, epsilon: 0.6907624792853566\n","Episode: 7600, Mean Reward: 44.05, epsilon: 0.6873859380048232\n","Episode: 7700, Mean Reward: 50.18, epsilon: 0.6838891067986339\n","Episode: 7800, Mean Reward: 46.25, epsilon: 0.6804440866804443\n","Episode: 7900, Mean Reward: 43.77, epsilon: 0.6770502730057991\n","Episode: 8000, Mean Reward: 43.3, epsilon: 0.6736733864695973\n","Episode: 8100, Mean Reward: 46.86, epsilon: 0.6704138997022131\n","Episode: 8200, Mean Reward: 42.89, epsilon: 0.666970057527371\n","Episode: 8300, Mean Reward: 50.64, epsilon: 0.6636434475293943\n","Episode: 8400, Mean Reward: 50.95, epsilon: 0.6603334294817664\n","Episode: 8500, Mean Reward: 48.28, epsilon: 0.657039920629726\n","Episode: 8600, Mean Reward: 55.33, epsilon: 0.6537955284076836\n","Episode: 8700, Mean Reward: 52.57, epsilon: 0.6505021015550608\n","Episode: 8800, Mean Reward: 48.03, epsilon: 0.6473223584959896\n","Episode: 8900, Mean Reward: 46.74, epsilon: 0.6440615395623299\n","Episode: 9000, Mean Reward: 48.89, epsilon: 0.6408171466642899\n","Episode: 9100, Mean Reward: 51.37, epsilon: 0.6376209781063321\n","Episode: 9200, Mean Reward: 46.51, epsilon: 0.6345359265331423\n","Episode: 9300, Mean Reward: 53.45, epsilon: 0.6313079509423268\n","Episode: 9400, Mean Reward: 52.84, epsilon: 0.6281592108786536\n","Episode: 9500, Mean Reward: 58.76, epsilon: 0.6250574284846314\n","Episode: 9600, Mean Reward: 49.97, epsilon: 0.6218776713776856\n","Episode: 9700, Mean Reward: 54.39, epsilon: 0.6187759662025044\n","Episode: 9800, Mean Reward: 57.89, epsilon: 0.6157513048233191\n","Episode: 9900, Mean Reward: 58.51, epsilon: 0.6126188893069242\n","Episode: 10000, Mean Reward: 53.95, epsilon: 0.6095938433437524\n","Episode: 10100, Mean Reward: 50.5, epsilon: 0.6065534055443552\n","Episode: 10200, Mean Reward: 58.11, epsilon: 0.6035281323699176\n","Episode: 10300, Mean Reward: 52.08, epsilon: 0.600517948184671\n","Episode: 10400, Mean Reward: 51.42, epsilon: 0.5975227777300909\n","Episode: 10500, Mean Reward: 60.56, epsilon: 0.5945425461230158\n","Episode: 10600, Mean Reward: 60.08, epsilon: 0.5915475999948323\n","Episode: 10700, Mean Reward: 62.6, epsilon: 0.5886266017843252\n","Episode: 10800, Mean Reward: 59.4, epsilon: 0.5856614566093405\n","Episode: 10900, Mean Reward: 62.42, epsilon: 0.5827403850634769\n","Episode: 11000, Mean Reward: 64.0, epsilon: 0.579833882786053\n","Episode: 11100, Mean Reward: 58.89, epsilon: 0.5769418771107269\n","Episode: 11200, Mean Reward: 59.62, epsilon: 0.5740930003836102\n","Episode: 11300, Mean Reward: 61.46, epsilon: 0.5712581911023287\n","Episode: 11400, Mean Reward: 65.55, epsilon: 0.5683805374864661\n","Episode: 11500, Mean Reward: 69.93, epsilon: 0.5655173797508586\n","Episode: 11600, Mean Reward: 66.1, epsilon: 0.5627530536114836\n","Episode: 11700, Mean Reward: 67.25, epsilon: 0.5598902478273579\n","Episode: 11800, Mean Reward: 62.29, epsilon: 0.557097713926965\n","Episode: 11900, Mean Reward: 68.53, epsilon: 0.5544299858758455\n","Episode: 12000, Mean Reward: 63.71, epsilon: 0.5515819402592429\n","Episode: 12100, Mean Reward: 68.47, epsilon: 0.5488308452491127\n","Episode: 12200, Mean Reward: 70.35, epsilon: 0.5460661670511854\n","Episode: 12300, Mean Reward: 66.68, epsilon: 0.5433425827607546\n","Episode: 12400, Mean Reward: 69.98, epsilon: 0.5406325827424041\n","Episode: 12500, Mean Reward: 80.32, epsilon: 0.5379629973924743\n","Episode: 12600, Mean Reward: 73.49, epsilon: 0.5352530648457575\n","Episode: 12700, Mean Reward: 71.62, epsilon: 0.532610042974658\n","Episode: 12800, Mean Reward: 75.78, epsilon: 0.5299270753780739\n","Episode: 12900, Mean Reward: 80.5, epsilon: 0.5272839871505655\n","Episode: 13000, Mean Reward: 75.5, epsilon: 0.5247065510526753\n","Episode: 13100, Mean Reward: 78.83, epsilon: 0.5220372933033263\n","Episode: 13200, Mean Reward: 78.74, epsilon: 0.5194595294858917\n","Episode: 13300, Mean Reward: 76.0, epsilon: 0.5168428062307995\n","Episode: 13400, Mean Reward: 83.77, epsilon: 0.5142906922299536\n","Episode: 13500, Mean Reward: 72.48, epsilon: 0.5117000064539752\n","Episode: 13600, Mean Reward: 73.3, epsilon: 0.5091478283790776\n","Episode: 13700, Mean Reward: 82.68, epsilon: 0.5066083796628742\n","Episode: 13800, Mean Reward: 72.61, epsilon: 0.5041068021559483\n","Episode: 13900, Mean Reward: 76.37, epsilon: 0.5016426593017529\n","Episode: 14000, Mean Reward: 83.68, epsilon: 0.4990657763529198\n","Episode: 14100, Mean Reward: 77.54, epsilon: 0.4966262747208965\n","Episode: 14200, Mean Reward: 85.14, epsilon: 0.49412457160758927\n","Episode: 14300, Mean Reward: 75.24, epsilon: 0.49166005356573056\n","Episode: 14400, Mean Reward: 89.71, epsilon: 0.4892078276654241\n","Episode: 14500, Mean Reward: 83.43, epsilon: 0.48674349420619933\n","Episode: 14600, Mean Reward: 84.49, epsilon: 0.48436422557117054\n","Episode: 14700, Mean Reward: 82.52, epsilon: 0.4819001950382575\n","Episode: 14800, Mean Reward: 92.61, epsilon: 0.4794966478493713\n","Episode: 14900, Mean Reward: 94.72, epsilon: 0.4771050887010559\n","Episode: 15000, Mean Reward: 82.89, epsilon: 0.4747491952609815\n","Episode: 15100, Mean Reward: 85.4, epsilon: 0.47235769565598784\n","Episode: 15200, Mean Reward: 94.67, epsilon: 0.4700017430682273\n","Episode: 15300, Mean Reward: 81.82, epsilon: 0.46768092518231336\n","Episode: 15400, Mean Reward: 87.67, epsilon: 0.46537156724466344\n","Episode: 15500, Mean Reward: 89.74, epsilon: 0.46300415509838394\n","Episode: 15600, Mean Reward: 94.37, epsilon: 0.46071789054670376\n","Episode: 15700, Mean Reward: 96.87, epsilon: 0.4584429153226521\n","Episode: 15800, Mean Reward: 101.84, epsilon: 0.456110750225982\n","Episode: 15900, Mean Reward: 95.59, epsilon: 0.45385852456364356\n","Episode: 16000, Mean Reward: 95.29, epsilon: 0.45161742014857137\n","Episode: 16100, Mean Reward: 99.5, epsilon: 0.4493424444505087\n","Episode: 16200, Mean Reward: 102.85, epsilon: 0.44712363995167165\n","Episode: 16300, Mean Reward: 92.27, epsilon: 0.44484905763806465\n","Episode: 16400, Mean Reward: 89.61, epsilon: 0.442630308376343\n","Episode: 16500, Mean Reward: 92.64, epsilon: 0.4404226254487\n","Episode: 16600, Mean Reward: 110.11, epsilon: 0.43822595366018774\n","Episode: 16700, Mean Reward: 101.7, epsilon: 0.43604023809115106\n","Episode: 16800, Mean Reward: 102.74, epsilon: 0.43386542409585466\n","Episode: 16900, Mean Reward: 98.72, epsilon: 0.43172304345328943\n","Episode: 17000, Mean Reward: 98.38, epsilon: 0.4295697620930543\n","Episode: 17100, Mean Reward: 111.97, epsilon: 0.427427220536234\n","Episode: 17200, Mean Reward: 109.44, epsilon: 0.4252953652164997\n","Episode: 17300, Mean Reward: 105.37, epsilon: 0.4231529841275507\n","Episode: 17400, Mean Reward: 96.62, epsilon: 0.42106350035749046\n","Episode: 17500, Mean Reward: 111.02, epsilon: 0.41896338501608354\n","Episode: 17600, Mean Reward: 112.35, epsilon: 0.4168945890343052\n","Episode: 17700, Mean Reward: 116.47, epsilon: 0.41479452598006145\n","Episode: 17800, Mean Reward: 108.32, epsilon: 0.41272567805854193\n","Episode: 17900, Mean Reward: 110.83, epsilon: 0.41068768320056287\n","Episode: 18000, Mean Reward: 118.1, epsilon: 0.4086597517412575\n","Episode: 18100, Mean Reward: 116.88, epsilon: 0.40658084076331374\n","Episode: 18200, Mean Reward: 116.53, epsilon: 0.4045529597893414\n","Episode: 18300, Mean Reward: 119.6, epsilon: 0.4025351931661508\n","Episode: 18400, Mean Reward: 121.05, epsilon: 0.40050746407242643\n","Episode: 18500, Mean Reward: 124.78, epsilon: 0.3985298014365532\n","Episode: 18600, Mean Reward: 122.24, epsilon: 0.3965420761901375\n","Episode: 18700, Mean Reward: 122.39, epsilon: 0.3945445367987316\n","Episode: 18800, Mean Reward: 122.13, epsilon: 0.39257668863830963\n","Episode: 18900, Mean Reward: 120.64, epsilon: 0.3906186554062958\n","Episode: 19000, Mean Reward: 123.57, epsilon: 0.3886898226404416\n","Episode: 19100, Mean Reward: 132.18, epsilon: 0.3867318381581326\n","Episode: 19200, Mean Reward: 121.97, epsilon: 0.3848029569664909\n","Episode: 19300, Mean Reward: 133.92, epsilon: 0.38288369634984293\n","Episode: 19400, Mean Reward: 138.39, epsilon: 0.38099305797707295\n","Episode: 19500, Mean Reward: 128.06, epsilon: 0.3790927997847864\n","Episode: 19600, Mean Reward: 124.94, epsilon: 0.3771831593051582\n","Episode: 19700, Mean Reward: 122.37, epsilon: 0.37530190353564946\n","Episode: 19800, Mean Reward: 135.19, epsilon: 0.3734487032375888\n","Episode: 19900, Mean Reward: 132.54, epsilon: 0.37158607360991525\n","Episode: 20000, Mean Reward: 136.37, epsilon: 0.3697327341179413\n","Episode: 20100, Mean Reward: 136.37, epsilon: 0.36790703377745504\n","Episode: 20200, Mean Reward: 130.18, epsilon: 0.366035437741575\n","Episode: 20300, Mean Reward: 127.92, epsilon: 0.3642097828518801\n","Episode: 20400, Mean Reward: 141.89, epsilon: 0.36239323368101084\n","Episode: 20500, Mean Reward: 150.39, epsilon: 0.3605857448128725\n","Episode: 20600, Mean Reward: 140.81, epsilon: 0.3587872710578896\n","Episode: 20700, Mean Reward: 146.22, epsilon: 0.3570156182327882\n","Episode: 20800, Mean Reward: 144.03, epsilon: 0.355217189254913\n","Episode: 20900, Mean Reward: 146.56, epsilon: 0.35344549195022545\n","Episode: 21000, Mean Reward: 135.73, epsilon: 0.35170021625388687\n","Episode: 21100, Mean Reward: 132.77, epsilon: 0.3499810575934709\n","Episode: 21200, Mean Reward: 141.91, epsilon: 0.34818324354595254\n","Episode: 21300, Mean Reward: 157.52, epsilon: 0.34646395226434623\n","Episode: 21400, Mean Reward: 163.3, epsilon: 0.3447359130000142\n","Episode: 21500, Mean Reward: 141.46, epsilon: 0.3429993417577745\n","Episode: 21600, Mean Reward: 140.37, epsilon: 0.3413056480237651\n","Episode: 21700, Mean Reward: 162.17, epsilon: 0.3396033365507569\n","Episode: 21800, Mean Reward: 163.05, epsilon: 0.3379095156033756\n","Episode: 21900, Mean Reward: 152.08, epsilon: 0.33620733162675015\n","Episode: 22000, Mean Reward: 161.92, epsilon: 0.3345471761057918\n","Episode: 22100, Mean Reward: 159.06, epsilon: 0.33286192956404903\n","Episode: 22200, Mean Reward: 158.06, epsilon: 0.3312182932774493\n","Episode: 22300, Mean Reward: 164.33, epsilon: 0.32954981563610386\n","Episode: 22400, Mean Reward: 158.35, epsilon: 0.3279061380839351\n","Episode: 22500, Mean Reward: 173.09, epsilon: 0.32627065861220006\n","Episode: 22600, Mean Reward: 168.45, epsilon: 0.32464333633178233\n","Episode: 22700, Mean Reward: 178.29, epsilon: 0.3230564353934041\n","Episode: 22800, Mean Reward: 151.99, epsilon: 0.321429072260731\n","Episode: 22900, Mean Reward: 178.69, epsilon: 0.3198258980951815\n","Episode: 23000, Mean Reward: 161.29, epsilon: 0.31824663232516065\n","Episode: 23100, Mean Reward: 186.38, epsilon: 0.31664349807428993\n","Episode: 23200, Mean Reward: 171.75, epsilon: 0.31504843944517064\n","Episode: 23300, Mean Reward: 164.29, epsilon: 0.3134770896121802\n","Episode: 23400, Mean Reward: 170.06, epsilon: 0.31194477081184224\n","Episode: 23500, Mean Reward: 168.96, epsilon: 0.3103578628626506\n","Episode: 23600, Mean Reward: 166.0, epsilon: 0.30880990796138097\n","Episode: 23700, Mean Reward: 170.15, epsilon: 0.30728503796182405\n","Episode: 23800, Mean Reward: 157.73, epsilon: 0.30576769760576494\n","Episode: 23900, Mean Reward: 184.97, epsilon: 0.304242636820151\n","Episode: 24000, Mean Reward: 176.77, epsilon: 0.3027100462375011\n","Episode: 24100, Mean Reward: 184.82, epsilon: 0.3012002359320207\n","Episode: 24200, Mean Reward: 186.32, epsilon: 0.2996979560246453\n","Episode: 24300, Mean Reward: 185.11, epsilon: 0.29820316895642096\n","Episode: 24400, Mean Reward: 194.77, epsilon: 0.29670100156385676\n","Episode: 24500, Mean Reward: 176.2, epsilon: 0.29522116224112743\n","Episode: 24600, Mean Reward: 170.99, epsilon: 0.2937927705440478\n","Episode: 24700, Mean Reward: 193.01, epsilon: 0.29231282007394777\n","Episode: 24800, Mean Reward: 185.95, epsilon: 0.2908403247001386\n","Episode: 24900, Mean Reward: 187.94, epsilon: 0.28938971635416716\n","Episode: 25000, Mean Reward: 190.64, epsilon: 0.2879751399219865\n","Episode: 25100, Mean Reward: 176.75, epsilon: 0.2865101689352235\n","Episode: 25200, Mean Reward: 200.55, epsilon: 0.2850669038119228\n","Problem Solved!\n"]}]},{"cell_type":"code","source":["env.close()"],"metadata":{"id":"4ZzxGVTCJCLQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"w9B8RzU3JCNu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pZZe5zmGJCQA"},"execution_count":null,"outputs":[]}]}